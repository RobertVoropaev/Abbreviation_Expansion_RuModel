{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import string \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "import pymorphy2\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import gensim\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import time\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src import *\n",
    "\n",
    "SEED = 1\n",
    "def init_random_seed(value=0):\n",
    "    random.seed(value)\n",
    "    np.random.seed(value)\n",
    "    torch.manual_seed(value)\n",
    "    torch.cuda.manual_seed(value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "init_random_seed(SEED)\n",
    "    \n",
    "pd.set_option('display.max_colwidth', 255)\n",
    "tqdm.pandas()\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=8, use_memory_fs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr = pd.read_csv(\"../data/abbr.csv\")\n",
    "lenta_train = pd.read_csv(\"../data/lenta_train.csv\")\n",
    "lenta_test = pd.read_csv(\"../data/lenta_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2abbr = {}\n",
    "abbr2id = {}\n",
    "id2desc = {}\n",
    "desc2id = {}\n",
    "for idx, abbr_name, desc in abbr[[\"abbr_id\", \"abbr_norm\", \"desc_norm\"]].values:\n",
    "    id2abbr[idx] = abbr_name\n",
    "    abbr2id[abbr_name] = idx\n",
    "    id2desc[idx] = desc\n",
    "    desc2id[desc] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenta_train[\"labels_new\"] = (\n",
    "    lenta_train[\"labels_new\"].str.replace(\"W\", \"\")\n",
    "                            .str.replace(\"B\", \"\")\n",
    "                            .str.replace(\"E\", \"\")\n",
    "                            .str.replace(\"-\", \"\")\n",
    ")\n",
    "train_texts = list(map(lambda x: x.split(\" \"), lenta_train[\"text_new\"].to_list()))\n",
    "train_labels = list(map(lambda x: x.split(\" \"), lenta_train[\"labels_new\"].to_list()))\n",
    "\n",
    "lenta_test[\"labels_new\"] = (\n",
    "    lenta_test[\"labels_new\"].str.replace(\"W\", \"\")\n",
    "                            .str.replace(\"B\", \"\")\n",
    "                            .str.replace(\"E\", \"\")\n",
    "                            .str.replace(\"-\", \"\")\n",
    ")\n",
    "test_texts = list(map(lambda x: x.split(\" \"), lenta_test[\"text_new\"].to_list()))\n",
    "test_labels = list(map(lambda x: x.split(\" \"), lenta_test[\"labels_new\"].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 403411 100853 635711 1162\n"
     ]
    }
   ],
   "source": [
    "PAD_TOKEN = \"<PAD>\"\n",
    "PAD_TOKEN_ID = 0\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize\n",
    "PAD_LABEL = \"<NOLABEL>\"\n",
    "PAD_LABEL_ID = 0\n",
    "\n",
    "EMPTY_LABEL = \"_\"\n",
    "EMPTY_LABEL_ID = 1\n",
    "\n",
    "train_texts_global = list(itertools.chain(*train_texts))\n",
    "train_labels_global = list(itertools.chain(*train_labels))\n",
    "train_labels_global = list(filter(lambda x: x != EMPTY_LABEL, train_labels_global))\n",
    "\n",
    "UNIQUE_TOKENS = [PAD_TOKEN] + list(set(train_texts_global))\n",
    "UNIQUE_LABELS = [PAD_LABEL, EMPTY_LABEL] + list(set(train_labels_global))\n",
    "\n",
    "token2id = {label: i for i, label in enumerate(UNIQUE_TOKENS)}\n",
    "id2token = {i: label for label, i in token2id.items()}\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(UNIQUE_LABELS)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "MAX_SENTENCE_LEN = lenta_train.text_new.str.split(\" \").str.len().max()\n",
    "train_size = len(train_texts)\n",
    "test_size = len(test_texts)\n",
    "TOKENS_NUM = len(UNIQUE_TOKENS)\n",
    "LABELS_NUM = len(UNIQUE_LABELS)\n",
    "\n",
    "print(MAX_SENTENCE_LEN, train_size, test_size, TOKENS_NUM, LABELS_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = os.path.join(\"../models/w2v\", \"emb_64.word2vec\")\n",
    "model = gensim.models.Word2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lev_dist(a, b):\n",
    "    a, b = list(a), list(b)\n",
    "    def recursive(i, j):\n",
    "        if i == 0 or j == 0:\n",
    "            return max(i, j)\n",
    "        elif a[i - 1] == b[j - 1]:\n",
    "            return recursive(i - 1, j - 1)\n",
    "        else:\n",
    "            return 1 + min(\n",
    "                recursive(i, j - 1),\n",
    "                recursive(i - 1, j),\n",
    "                recursive(i - 1, j - 1)\n",
    "            )\n",
    "    return recursive(len(a), len(b))\n",
    "\n",
    "\n",
    "def get_desc_find_by_dist(word, w2v_model, topn=5, dist=prefix_dist):\n",
    "    desc_score_dist = []\n",
    "    for desc, score in w2v_model.wv.most_similar(word, topn=topn):\n",
    "        desc = desc.replace(\"_\", \" \")\n",
    "        desc_score_dist.append([desc, score, dist(word, desc)])\n",
    "    desc_score_dist = sorted(desc_score_dist, key=lambda x: x[2])\n",
    "    return desc_score_dist[0][0]\n",
    "\n",
    "def get_desc_first(word, w2v_model):\n",
    "    first_desc = w2v_model.wv.most_similar(word, topn=1)[0][0]\n",
    "    first_desc = first_desc.replace(\"_\", \" \")\n",
    "    return first_desc\n",
    "\n",
    "def get_desc_find_in_dict(word, w2v_model, desc2id):\n",
    "    for desc, score in w2v_model.wv.most_similar(word, topn=10):\n",
    "        desc = desc.replace(\"_\", \" \")\n",
    "        if desc in desc2id.keys():\n",
    "            return desc\n",
    "        \n",
    "    return get_desc_first(word, w2v_model)\n",
    "        \n",
    "\n",
    "def get_token2desc(tokens, w2v_model, desc2id, \n",
    "                   get_desc_f=get_desc_find_by_dist, topn=5, dist=intersection_dist):\n",
    "    abbr_detection = AbbrDetection()\n",
    "\n",
    "    token2desc = {}\n",
    "    for token in tqdm(tokens):\n",
    "        if abbr_detection.word_is_abbr(token):\n",
    "            if token in w2v_model.wv.key_to_index:\n",
    "                desc = get_desc_find_by_dist(token, w2v_model, \n",
    "                                             topn=topn, dist=intersection_dist)\n",
    "                label = desc2id.get(desc, \"_\")\n",
    "            else:\n",
    "                label = \"_\"\n",
    "        else:\n",
    "            label = \"_\"\n",
    "        token2desc[token] = label\n",
    "    return token2desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenta_test.sample(1000).to_csv(\"../lenta_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 635711/635711 [03:53<00:00, 2717.40it/s]\n"
     ]
    }
   ],
   "source": [
    "token2desc = get_token2desc(tokens=list(token2id.keys()), \n",
    "                            w2v_model=model, \n",
    "                            desc2id=desc2id, \n",
    "                            topn=10, \n",
    "                            dist=lev_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../desc2id.pickle\", \"wb\") as f: \n",
    "    pickle.dump(desc2id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../token2desc.pickle\", \"wb\") as f: \n",
    "    pickle.dump(token2desc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ЦБ прорабатывает вопрос включения финграмотности в нацпроекты\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(line):\n",
    "    return word_tokenize(line)\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer(lang=\"ru\", \n",
    "                                units=[pymorphy2.units.DictionaryAnalyzer()])\n",
    "def normalize(word):\n",
    "    word = str(word).lower()\n",
    "    parse_list = morph.parse(word)\n",
    "    if parse_list != []:\n",
    "        return parse_list[0].normal_form\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def get_tokenized_normal_form(line):\n",
    "    line = str(line)\n",
    "    new_line_arr = []\n",
    "    for token in tokenize(line):\n",
    "        new_line_arr.append(normalize(token))\n",
    "    return \" \".join(new_line_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(norm_text):\n",
    "    preds = []\n",
    "    for word in norm_text:\n",
    "        if word in token2desc:\n",
    "            label = token2desc[word]  \n",
    "        else:\n",
    "            label = \"_\"\n",
    "        preds.append(str(label))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_text(text, preds):\n",
    "    new_text = []\n",
    "    for i in range(len(preds)):\n",
    "        label = preds[i]\n",
    "        if label == \"_\":\n",
    "            new_text.append(text[i])\n",
    "        else:\n",
    "            new_text.append(id2desc[int(label)])\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пер\t692\tпериод\n",
      "мн\t928\tмеждународный\n",
      "млн\t1037\tмиллион\n"
     ]
    }
   ],
   "source": [
    "# text = test_texts[7]\n",
    "preds = get_preds(text)\n",
    "new_text = get_new_text(text, preds)\n",
    "\n",
    "res = []\n",
    "for text_word, label, text_upd in zip(text, preds, new_text):\n",
    "    if label != \"_\":\n",
    "        print(text_word, label, text_upd, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"в пер холод перебор газ украина достигать 80 миллион кубометр в сутки , передавать риа новость . в сообщение , обнародовать `` газпром '' по итог состояться в четверг совещание , указываться , что из-за это с 19 по 25 январь европа недополучить 326 миллион кубометр газ . `` в ситуация , когда сильный мороз обрушиться практически на всё европейский страна , украина , пользоваться свой положение страны-транзитера , обеспечивать себя газ за счёт потребитель в европа , незаконно превышать не только собственный лимит потребление , но и забирать дополнительный поставка российский газ в европа , - заявить в ход это совещание заместитель председатель правление компания александр ананенков . - наступить холод показать , что украина являться единственный транзитный государство , который грубый образ попирать мн норма ведение газовый бизнес . фактически это означать полный отсутствие контроль в энергетический сфера украина '' . в сообщение также отмечаться , что с 16 по 25 январь `` газпром '' дополнительно поставить на внутренний рынок 2,2 миллиард кубометр газ . `` при это промышленный потребитель по-прежнему не выполнять график переход на резервный вид топливо . из 211,5 миллион кубометр газ , подлежащее высвобождение , реально быть высвободить не более 45 млн кубометр в сутки '' , - отмечаться в документ . по итог совещание быть принять решение сократить список регион , в который ввести режим переход на резервный топливо , в связь с потепление .\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\" \".join(text))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = []\n",
    "for dist in [lev_dist]:\n",
    "    for topn in [50]:\n",
    "        print(dist.__name__, topn)\n",
    "        token2desc = get_token2desc(tokens=list(token2id.keys()), \n",
    "                                    w2v_model=model, \n",
    "                                    desc2id=desc2id, \n",
    "                                    topn=topn, \n",
    "                                    dist=dist)\n",
    "\n",
    "        preds = []\n",
    "        for text in tqdm(test_texts):\n",
    "            labels = []\n",
    "            for word in text:\n",
    "                if word in token2desc:\n",
    "                    label = token2desc[word]  \n",
    "                else:\n",
    "                    label = \"_\"\n",
    "                labels.append(str(label))\n",
    "            preds.append(labels)\n",
    "\n",
    "        test_labels_global = list(itertools.chain(*test_labels))\n",
    "        test_preds_global = list(itertools.chain(*preds))\n",
    "\n",
    "        test_labels_global_upd = []\n",
    "        for label_id in test_labels_global:\n",
    "            if label_id == \"_\":\n",
    "                test_labels_global_upd.append(\"_\")\n",
    "            else:\n",
    "                test_labels_global_upd.append(id2desc.get(int(label_id), \"_\"))\n",
    "\n",
    "        test_preds_global_upd = []\n",
    "        for label_id in test_preds_global:\n",
    "            if label_id == \"_\":\n",
    "                test_preds_global_upd.append(\"_\")\n",
    "            else:\n",
    "                test_preds_global_upd.append(id2desc.get(int(label_id), \"_\"))\n",
    "\n",
    "        f1 = f1_score(test_labels_global_upd, test_preds_global_upd, average=\"macro\")\n",
    "        filtred_acc = get_filtred_accuracy_score(test_labels_global_upd, test_preds_global_upd)\n",
    "        print(f1, filtred_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
